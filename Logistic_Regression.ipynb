{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9770073",
   "metadata": {},
   "source": [
    "Difference between Linear Regression and Logistic Regression:\n",
    "\n",
    "Linear regression is used for predicting continuous numeric values, whereas logistic regression is used for predicting the probability of a binary outcome. In linear regression, the output variable is continuous, and the relationship between the independent variables and the dependent variable is assumed to be linear. In logistic regression, the output variable is binary (e.g., yes/no, 0/1), and the relationship between the independent variables and the log-odds of the outcome is modeled using the logistic function. An example scenario where logistic regression would be more appropriate is predicting whether a customer will churn or not (binary outcome), based on factors like customer demographics, purchase history, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef0dcc",
   "metadata": {},
   "source": [
    "Cost Function and Optimization in Logistic Regression:\n",
    "\n",
    "The cost function used in logistic regression is the binary cross-entropy (or log loss) function. It measures the difference between the predicted probability and the actual outcome for each observation in the dataset. The goal is to minimize this cost function to optimize the logistic regression model. Optimization is typically done using gradient descent or its variants, where the model parameters (coefficients) are iteratively updated in the direction that minimizes the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4061342c",
   "metadata": {},
   "source": [
    "Regularization in Logistic Regression:\n",
    "\n",
    "Regularization in logistic regression involves adding a penalty term to the cost function to discourage overly complex models. This helps prevent overfitting by penalizing large coefficients. Common types of regularization in logistic regression include L1 regularization (Lasso) and L2 regularization (Ridge). These techniques shrink the coefficients towards zero, effectively reducing the model's complexity and preventing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e025439c",
   "metadata": {},
   "source": [
    "ROC Curve for Evaluating Logistic Regression:\n",
    "\n",
    "The ROC (Receiver Operating Characteristic) curve is a graphical representation of the trade-off between true positive rate (sensitivity) and false positive rate (1-specificity) for different threshold values of the predicted probabilities. It is used to evaluate the performance of a logistic regression model in distinguishing between the two classes. A higher area under the ROC curve (AUC) indicates better discrimination ability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca1c3a1",
   "metadata": {},
   "source": [
    "Feature Selection Techniques in Logistic Regression:\n",
    "\n",
    "Common techniques for feature selection in logistic regression include:\n",
    "Univariate feature selection: Selecting features based on statistical tests like chi-square test or ANOVA.\n",
    "Recursive feature elimination: Iteratively removing the least important features based on model performance.\n",
    "Regularization: Including penalty terms in the cost function to automatically select relevant features while training the model.\n",
    "These techniques help improve the model's performance by reducing overfitting, decreasing training time, and enhancing interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129f2ba9",
   "metadata": {},
   "source": [
    "Handling Imbalanced Datasets in Logistic Regression:\n",
    "\n",
    "Imbalanced datasets occur when one class (e.g., positive cases) is significantly more prevalent than the other class (e.g., negative cases). Strategies for handling imbalanced datasets in logistic regression include:\n",
    "Resampling techniques: Oversampling the minority class or undersampling the majority class to balance the dataset.\n",
    "Synthetic data generation: Creating artificial samples for the minority class using techniques like SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "Class weights: Assigning higher weights to minority class samples during model training to give them more importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ebac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
